{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "ipdfuskmvkgjmwrscpdp",
   "authorId": "150479669295",
   "authorName": "ECASTRO",
   "authorEmail": "edison.castro@snowflake.com",
   "sessionId": "17956814-8f0d-4739-98b4-f927fc4b52fa",
   "lastEditTime": 1753218534844
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e63035-7055-498d-b71a-dbcb3c98f296",
   "metadata": {
    "name": "Introduction",
    "collapsed": true
   },
   "source": "# User Engagement WIP....\n\nThe purpose of this notebook is to identify areas where customers ....\n\nUsing this notebook, you'll be able to:\n- Identify ...\n- Analyze....\n- Recommend ....\n\n### How to use this notebook\n\nBefore doing anything, **make a copy** of this notebook in your personal temp space. Don't modify the main copy, as it will be used by other SAs and changes could overwrite each other.\n\nOnce you've made a copy, you can modify variables and set up the environment for your customer using the sections below:\n\n1. Run the `init_` cells to create connections and variables.\n2. Use the `config_` section to set the notebook context for your customer account.\n3. Run the `cache_` cells to generate all the data needed for the analysis.\n\nThe notebook then begins in earnest with the `Overview` section.\n\n### Sharing with customers\n\nAll the queries used in this notebook are built using customer-visible data, assuming they are in the PrPr for warehouse utilization views. They can be freely shared with customers to enable them to do their own analysis. To share, simply replace `PST.SVCS` with `SNOWFLAKE.ACCOUNT_USAGE` in all the scripts. If any script requires further customization, it will be explained in that section.\n\n**Note:** All the queries will focus on an Account, rather than an Organization, because that is how the relevant metrics are presented to the customer in the product.\n"
  },
  {
   "cell_type": "code",
   "id": "8b9cdc49-6509-47d7-b960-d3c6ea71f06b",
   "metadata": {
    "language": "sql",
    "name": "init_sql",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--use role technical_account_manager;\nuse role SUPPORT_ENGINEER;\nuse database snowhouse_import;\n--use warehouse doubleblackdiamonds;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "init_python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "source": "import streamlit as st\nimport pandas as pd\nimport altair as alt\nimport plotly.graph_objects as go\n\n#from streamlit_extras import stylable_container\nfrom snowflake.snowpark.context import get_active_session\nfrom datetime import datetime, date, time\nsession = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "d1674fb5-fa1b-4f50-a91a-9b39602081ec",
   "metadata": {
    "language": "sql",
    "name": "init_variables",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "set top_x_wh = 30;\nset lookback_days = 15;\nset start_time = dateadd(day,-$lookback_days,current_date);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "04810efb-efec-4f3c-bbf6-760ff6f0fa36",
   "metadata": {
    "name": "Configure_Account",
    "collapsed": false
   },
   "source": "### Find Your Account\n\nYou will need to set the notebook context for the account you wish to analyze. Enter a name or search value into `config_acct_set` and run it, then run `config_acct_search`. It will return all matching results from account names, aliases, organizations, and salesforce names. The results are sorted by most recent credit usage, so your account is likely in the top few results. If it's not, try a different search value.\n\nIf you already know your Account ID and Deployment, you may skip the search steps."
  },
  {
   "cell_type": "code",
   "id": "be486b50-9a57-42f5-b6c6-2839e2dda54f",
   "metadata": {
    "language": "python",
    "name": "GetYourCustomer",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# import pandas as pd\n# import streamlit as st\n# session = get_active_session()\n@st.cache_data(ttl=1600)\ndef fetch_priority_accounts(_session):\n    query = \"\"\"\n        SELECT NAME, ORG_ID_C \n        FROM FIVETRAN.SALESFORCE.ACCOUNT \n        WHERE PRIORITY_SUPPORT_C = TRUE AND IS_DELETED = FALSE\n        ORDER BY NAME\n    \"\"\"\n    return pd.DataFrame(session.sql(query).collect())\n\n@st.cache_data(ttl=1600)\ndef fetch_accounts_by_orgid(_session, org_id):\n    query = f\"\"\"\n        SELECT ID, ALIAS, NAME, DEPLOYMENT\n        FROM SNOWHOUSE_IMPORT.PROD.ACCOUNT_ETL_V\n        WHERE REPLICATION_GROUP = '{org_id}' ORDER BY NAME\n    \"\"\"\n    return pd.DataFrame(session.sql(query).collect())\n\n\nst.write(\"**************\")\npriority_accounts = fetch_priority_accounts(session)\ncustomer_name = st.selectbox(\n    \"Please Select Your Priority Support Customer\", \n    priority_accounts['NAME'])\n\norg_id = priority_accounts.loc[priority_accounts['NAME'] == customer_name, 'ORG_ID_C'].values[0]\nst.write(\"Selected ORG_ID_C:\", org_id)\nst.write(\"**************\")\n# accounts = fetch_accounts_by_orgid(session, org_id)\n# st.write(accounts)\n# account_name = st.selectbox(\n#     \"Please Select The Snowflake Account\", \n#     accounts['NAME'].unique())\n# st.write(\"**************\")\n# account_rows = accounts[accounts['NAME'] == account_name]\n\n# if len(account_rows) > 1:\n#     deployment = st.selectbox(\"Please Select The Snowflake Deployment\", account_rows['DEPLOYMENT'])\n#     selected_row = account_rows[account_rows['DEPLOYMENT'] == deployment].iloc[0]\n# else:\n#     selected_row = account_rows.iloc[0]\n\n# acc_id = selected_row['ID']\n# acc_id = int(acc_id)\n# acc_dep = selected_row['DEPLOYMENT']\n# #acc_dep = acc_dep.iloc[0]\n# acc_name = selected_row['NAME']\n# #acc_name = acc_name\n# st.write(f\"Account ID: {acc_id}, Deployment: {acc_dep}, Name: {acc_name}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8405712-070e-4904-8f7f-0fad8a47ac28",
   "metadata": {
    "language": "python",
    "name": "GetYourAccount",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "accounts = fetch_accounts_by_orgid(session, org_id)\nst.write(accounts)\naccount_name = st.selectbox(\n    \"Please Select The Snowflake Account\", \n    accounts['NAME'].unique())\nst.write(\"**************\")\naccount_rows = accounts[accounts['NAME'] == account_name]\n\nif len(account_rows) > 1:\n    deployment = st.selectbox(\"Please Select The Snowflake Deployment\", account_rows['DEPLOYMENT'])\n    selected_row = account_rows[account_rows['DEPLOYMENT'] == deployment].iloc[0]\nelse:\n    selected_row = account_rows.iloc[0]\n\nacc_id = selected_row['ID']\nacc_id = int(acc_id)\nacc_dep = selected_row['DEPLOYMENT']\n#acc_dep = acc_dep.iloc[0]\nacc_name = selected_row['NAME']\n#acc_name = acc_name\nst.write(f\"Account ID: {acc_id}, Deployment: {acc_dep}, Name: {acc_name}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e19e4c69-b5fc-494e-af4e-293a42e56a19",
   "metadata": {
    "language": "sql",
    "name": "config_acct_search",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "set acct_search = '{{acc_name}}';\n--select '{{accName}};\n--set acct_search = 'EY13321';\n\nselect\n    m.salesforce_account_name,\n    m.organization_name,\n    m.snowflake_account_name,\n    m.snowflake_account_alias,\n    m.snowflake_deployment,\n    m.snowflake_account_id,\n    round(u.total_credits) daily_credits,\n    round(u.daily_storage_tb) daily_storage_tb,\nfrom finance.customer.salesforce_snowflake_mapping m\njoin finance.customer.usage_daily u on m.snowflake_deployment = u.snowflake_deployment and m.snowflake_account_id = u.snowflake_account_id\nwhere (m.salesforce_account_name ilike '%'|| $acct_search ||'%' or m.snowflake_account_name ilike '%'|| $acct_search ||'%' or m.organization_name ilike '%'|| $acct_search ||'%')\n  and u.usage_date = dateadd(day,-1,current_date)\norder by daily_credits desc nulls last;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cac29172-c6db-4c42-821d-c6f2c454d902",
   "metadata": {
    "name": "Config_Acct_Context",
    "collapsed": false
   },
   "source": "### Set Account ID and Deployment\n\nWhen you've identified your account, copy __snowflake_account_id__ and __snowflake_deployment__ from the results above into `config_acct_context` and run it."
  },
  {
   "cell_type": "code",
   "id": "142a28d7-65e9-4072-ae92-e24095335e66",
   "metadata": {
    "language": "sql",
    "name": "config_acct_context",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "--call pst.svcs.sp_set_account_context (329091,'va2'); --Snowhouse\n\n--call pst.svcs.sp_set_account_context (4329,'va');--Fidelity Production\n--call pst.svcs.sp_set_account_context (192678,'azeastus2prod'); --Carmax\n--call pst.svcs.sp_set_account_context (263303,'va3'); --Cargill, Inc263303\n--call pst.svcs.sp_set_account_context (98070,'gcpuseast4'); --Solenis, Prod\ncall pst.svcs.sp_set_account_context ({{acc_id}},'{{acc_dep}}');",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "db201d55-abcf-482e-b4fa-0f41fb4655b6",
   "metadata": {
    "name": "Cache_Data",
    "collapsed": false
   },
   "source": "### Cache Data\n\nRun the cells below to query `QUERY_HISTORY` and `SESSIONS` views, and cache the data we'll be using for analysis."
  },
  {
   "cell_type": "code",
   "id": "d0d2c61a-9632-450a-8d10-f32bc63d27dc",
   "metadata": {
    "language": "sql",
    "name": "UserEngagement",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Set your context variables as appropriate:\n--set ACCOUNT_ID = <customer_account_id>;\n--set ACCOUNT_ID = {{accId}};\n\n-- Get user query and object access metrics for last 30 days:\nwith user_queries as (\n  select\n    user_name,\n    count(*) as total_queries,\n    max(start_time) as last_query_time\n  from\n    pst.svcs.QUERY_HISTORY\n  where\n    start_time >= dateadd('day', -30, current_timestamp())\n    --and account_id = $ACCOUNT_ID\n  group by\n    user_name\n),\nobject_access as (\n  select\n    user_name,\n    count(distinct BASE_OBJECTS_ACCESSED) as objects_accessed\n  from\n    pst.svcs.ACCESS_HISTORY\n  where\n    query_start_time >= dateadd('day', -30, current_timestamp())\n    --and account_id = $ACCOUNT_ID\n  group by\n    user_name\n)\nselect\n  q.user_name,\n  q.total_queries,\n  nvl(a.objects_accessed, 0) as objects_accessed,\n  q.last_query_time\nfrom\n  user_queries q\n  left join object_access a on q.user_name = a.user_name\norder by\n  q.total_queries desc;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7a80caa3-195d-4f13-9ca9-c5e8f8710ab9",
   "metadata": {
    "language": "python",
    "name": "UserTotQueriesandObjects"
   },
   "outputs": [],
   "source": "UserQandO = UserEngagement.to_pandas()\n#st.bar_chart(UserQandO, x=\"USER_NAME\", y=[\"TOTAL_QUERIES\",\"OBJECTS_ACCESSED\"],horizontal = True)\nst.bar_chart(UserQandO, x=\"USER_NAME\", y=[\"TOTAL_QUERIES\",\"OBJECTS_ACCESSED\"],horizontal = True)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ed38e718-e1de-433c-af39-f457b69b6b44",
   "metadata": {
    "language": "sql",
    "name": "UserEngagementQueries",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Set your context variables as appropriate:\n-- set ACCOUNT_ID = <customer_account_id>;\n\n-- Get user query and object access metrics for last 30 days:\n\nselect\nQUERY_TYPE, \nDATABASE_NAME, \nSCHEMA_NAME, \nUSER_NAME, \nROLE_NAME, \nWAREHOUSE_NAME,\nWAREHOUSE_SIZE, \nSUM(TOTAL_ELAPSED_TIME) as TOTAL_ELAPSED_TIME,\nSUM(COMPILATION_TIME) as TOTAL_COMPILATION_TIME, \nSUM(EXECUTION_TIME) as TOTAL_EXECUTION_TIME,\nCOUNT(QUERY_ID) as TOTAL_QUERIES,\nCOUNT(SESSION_ID) TOTAL_SESSIONS\nfrom\n    pst.svcs.QUERY_HISTORY\nwhere\n    start_time >= dateadd('day', -30, current_timestamp())\n    AND WAREHOUSE_NAME NOT ILIKE 'COMPUTE_%'\n    --and account_id = $ACCOUNT_ID\ngroup by ALL;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9132b87e-3f2e-440a-85be-a4e0c99831f6",
   "metadata": {
    "language": "python",
    "name": "SessionStats"
   },
   "outputs": [],
   "source": "SessStAllx = UserEngagementQueries.to_pandas()\nSessStAll = SessStAllx.fillna(\"Empty\")\nSessSt = UserEngagementQueries.to_pandas()\n\n\n#st.write(SessSt)\n\n#SessSt = SessStAll.loc[SessStAll['SCHEMA_NAME'].str.contains('WEATHER')]\n\nSessionsCount  = SessSt.groupby(['SCHEMA_NAME', 'ROLE_NAME', 'USER_NAME', 'QUERY_TYPE'],as_index=False).agg({'TOTAL_SESSIONS':['sum'],'TOTAL_QUERIES': ['mean','min','max','sum']})\nSessionsCount.columns = ['_'.join(col) for col in SessionsCount.columns.values]\nSessionsCount.rename(columns={\"ROLE_NAME_\": \"ROLE_NAME\"},inplace=True)\nSessionsCount.rename(columns={\"USER_NAME_\": \"USER_NAME\"},inplace=True)\nSessionsCount.rename(columns={\"QUERY_TYPE_\": \"QUERY_TYPE\"},inplace=True)\nSessionsCount.rename(columns={\"SCHEMA_NAME_\": \"SCHEMA_NAME\"},inplace=True)\nst.write(SessionsCount)\n\n\n\n# 1. Create a sample DataFrame\n# data = {\n#     'Source_Category': ['A', 'A', 'B', 'B', 'C', 'C'],\n#     'Target_Category': ['X', 'Y', 'X', 'Z', 'Y', 'Z'],\n#     'Value': [10, 5, 8, 12, 7, 3]\n# }\ndf = pd.DataFrame(SessionsCount)\n\n# 2. Prepare data for Sankey diagram\n# Create a list of unique labels from 'Source_Category' and 'Target_Category'\nlabels = list(pd.concat([df['ROLE_NAME'], df['SCHEMA_NAME']]).unique())\n\n\n# Map labels to indices, as Plotly's Sankey expects numerical indices for source and target\nlabel_to_index = {label: idx for idx, label in enumerate(labels)}\n\n# Create source, target, and value lists based on the DataFrame\nsources = df['ROLE_NAME'].map(label_to_index).tolist()\ntargets = df['SCHEMA_NAME'].map(label_to_index).tolist()\nvalues = df['TOTAL_QUERIES_sum'].tolist()\n\n# 3. Create the Sankey diagram\nfig = go.Figure(data=[go.Sankey(\n    node=dict(\n        pad=15,\n        thickness=20,\n        line=dict(color=\"black\", width=0.5),\n        label=labels,  # Use the unique labels for node labels\n    ),\n    link=dict(\n        source=sources,\n        target=targets,\n        value=values,\n    )\n)])\n\nfig.update_layout(title_text=\"Sample Sankey Diagram Role Sessions by Warehouse\",title_font_color=\"Black\", font_size=12,font_color=\"black\",)\n\n# fig.update_layout(\n#     title_text=\"Basic Sankey Diagram\",\n#     font_family=\"Courier New\",\n#     font_color=\"blue\",\n#     font_size=12,\n#     title_font_family=\"Times New Roman\",\n#     title_font_color=\"red\",\n# )\n# Display in Streamlit\nst.plotly_chart(fig)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4756335-6599-49ec-a153-bcc60393bc6f",
   "metadata": {
    "language": "sql",
    "name": "AllQueriesText"
   },
   "outputs": [],
   "source": "-- Set your context variables as appropriate:\n-- set ACCOUNT_ID = <customer_account_id>;\n\n-- Get user query and object access metrics for last 30 days:\n\nselect\nQUERY_TEXT,\nQUERY_TAG,\nQUERY_TYPE,\nSESSION_ID,\nDATABASE_NAME,\nSCHEMA_NAME, \nUSER_NAME, \nROLE_NAME, \nWAREHOUSE_NAME,\nWAREHOUSE_SIZE, \nis_client_generated_statement,\ntransaction_id,\nSUM(TOTAL_ELAPSED_TIME) as TOTAL_ELAPSED_TIME,\nSUM(COMPILATION_TIME) as TOTAL_COMPILATION_TIME, \nSUM(EXECUTION_TIME) as TOTAL_EXECUTION_TIME,\nCOUNT(QUERY_PARAMETERIZED_HASH) as TOTAL_QUERIES\nfrom\n    pst.svcs.QUERY_HISTORY\nwhere\n    start_time >= dateadd('day', -30, current_timestamp())\n    AND WAREHOUSE_NAME NOT ILIKE 'COMPUTE_%'\n    --and account_id = $ACCOUNT_ID\ngroup by ALL;\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "30b6d6c3-a07c-4e0a-a608-623868b20ffd",
   "metadata": {
    "language": "python",
    "name": "CategorizeJobs"
   },
   "outputs": [],
   "source": "AllQ = AllQueriesText.to_pandas()\n\nSessionsCount  = AllQ.groupby(['SESSION_ID','WAREHOUSE_NAME'],as_index=False).agg({'TOTAL_QUERIES':['sum'],'TOTAL_QUERIES': ['mean','min','max','sum']})\nSessionsCount.columns = ['_'.join(col) for col in SessionsCount.columns.values]\nSessionsCount.rename(columns={\"SESSION_ID_\": \"SESSION_ID\"},inplace=True)\nSessionsCount.rename(columns={\"WAREHOUSE_NAME_\": \"WH\"},inplace=True)\nst.write(SessionsCount)\n\npoints = alt.Chart(SessionsCount).mark_point().encode(\n             x='WH',\n             y='TOTAL_QUERIES_sum',\n             color='SESSION_ID',\n             size = 'TOTAL_QUERIES_sum',\n             tooltip=[\n                 alt.Tooltip(\"SESSION_ID\", title=\"SESSION_ID\"),\n                 alt.Tooltip(\"WH\", title=\"WH\"),\n             ],\n         ).interactive()\n\nchart = alt.vconcat(points, data=SessionsCount, title=\"Session Queries\")\nst.altair_chart(chart, theme=\"streamlit\", use_container_width=True)\n\n#st.write(AllQ)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "986b4cec-daf0-4465-886b-da094f87d773",
   "metadata": {
    "language": "python",
    "name": "UserEngagementInfo",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "df = UserEngagementQueries.to_pandas()\n\n#chart_data=pd.DataFrame(data=pd.pivot_table(df, index=['USER_NAME'], values=['TOTAL_QUERIES'], aggfunc='sum'))\n#st.write(chart_data)\n#st.bar_chart(chart_data)\n\nqueryCountH_df  = df.groupby(['WAREHOUSE_NAME','USER_NAME'],as_index=False).agg({'TOTAL_QUERIES':['sum'],'TOTAL_QUERIES': ['mean','min','max','sum']})\nqueryCountH_df.columns = ['_'.join(col) for col in queryCountH_df.columns.values]\nqueryCountH_df.rename(columns={\"USER_NAME_\": \"USER_NAME\"},inplace=True)\nqueryCountH_df.rename(columns={\"WAREHOUSE_NAME_\": \"WH\"},inplace=True)\nst.write(queryCountH_df)\n\npoints = alt.Chart(queryCountH_df).mark_point().encode(\n             x='WH',\n             y='TOTAL_QUERIES_sum',\n             color='USER_NAME',\n             size = 'TOTAL_QUERIES_sum',\n             tooltip=[\n                 alt.Tooltip(\"USER_NAME\", title=\"USER_NAME\"),\n                 alt.Tooltip(\"WH\", title=\"WH\"),\n             ],\n         ).interactive()\n\nchart = alt.vconcat(points, data=queryCountH_df, title=\"USER_NAME Cost\")\nst.altair_chart(chart, theme=\"streamlit\", use_container_width=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c12ec400-9baf-4b05-96a1-d85a7872cd66",
   "metadata": {
    "language": "sql",
    "name": "CreditsPerService",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT\n*\nFROM\n    pst.svcs.METERING_DAILY_HISTORY\nWHERE 1=1\n    --SERVICE_TYPE != 'WAREHOUSE_METERING'\n    AND USAGE_DATE >= DATEADD(month, -1, CURRENT_DATE)\nORDER BY\n    USAGE_DATE DESC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0da267cd-5da3-4d69-a714-5edeecaa6513",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CredPServ = CreditsPerService.to_pandas()\nst.bar_chart(CredPServ, x=\"USAGE_DATE\", y=\"CREDITS_USED\", color=\"SERVICE_TYPE\",horizontal = True)\n\n\n# Create an Altair bar chart\nchart = alt.Chart(CredPServ).mark_bar(size=40).encode(\n    x='USAGE_DATE',\n    y='CREDITS_USED',\n    color = 'SERVICE_TYPE'\n)\n\n\n# Display the chart in Streamlit\nst.altair_chart(chart, use_container_width=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4e9a75c-588c-471d-a6b0-bf1139b2ec1f",
   "metadata": {
    "language": "sql",
    "name": "AppSessions"
   },
   "outputs": [],
   "source": "select \n    qh.USER_NAME, \n    --qh.ROLE_NAME,\n    qh.query_type,\n    --se.CLIENT_APPLICATION_ID, \n    --se.CLIENT_APPLICATION_VERSION, \n    --se.CLIENT_ENVIRONMENT,\n    --COALESCE(se.CLIENT_ENVIRONMENT:APPLICATION::varchar) AS app,\n    PARSE_JSON(se.CLIENT_ENVIRONMENT):APPLICATION AS app,\n    COUNT (distinct qh.SESSION_ID) as TOTAL_SESSIONS,\n    \n    --count(distinct qh.query_text),\n    --count(distinct qh.USER_NAME) as users_per_session\nfrom\n    pst.svcs.QUERY_HISTORY qh\n    JOIN pst.svcs.SESSIONS se on (qh.SESSION_ID = se.SESSION_ID)\nWHERE \nqh.start_time >= dateadd('day', -3, current_timestamp())\nGROUP BY ALL ;\n-- where\n--     start_time >= dateadd('day', -30, current_timestamp());\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2724a551-765e-4240-b939-3d4ccc42b51e",
   "metadata": {
    "language": "python",
    "name": "UserAppQTypeSessions"
   },
   "outputs": [],
   "source": "\n# from streamlit_extras.stylable_container import stylable_container\n# with stylable_container(\n#     key=\"my_styled_container\",\n#     css_styles=\"\"\"\n#     {\n#         border: 2px solid green;\n#         border-radius: 10px;\n#         padding: 20px;\n#         .node-label-text-path {\n#         fill: rgb(250, 250, 250) !important;\n#         text-shadow: none\n#         }\n#     }\n#     \"\"\",\n# ):\n#     st.write(\"This text is inside a styled container.\")\n\n# st.write(\"This text is outside the styled container.\")\n\nappSess = AppSessions.to_pandas()\n\n\nappSess_df  = appSess.groupby(['APP','USER_NAME','QUERY_TYPE'],as_index=False).agg({'TOTAL_SESSIONS':['sum'],'TOTAL_SESSIONS': ['mean','min','max','sum']})\nappSess_df.columns = ['_'.join(col) for col in appSess_df.columns.values]\nappSess_df.rename(columns={\"USER_NAME_\": \"USER_NAME\"},inplace=True)\nappSess_df.rename(columns={\"APP_\": \"APP\"},inplace=True)\nappSess_df.rename(columns={\"QUERY_TYPE_\": \"QUERY_TYPE\"},inplace=True)\nst.write(appSess_df)\n#appSess_df2 = appSess_df.loc[appSess_df['USER_NAME'].str.contains('PUBLIC')]\nappSess_df2 = appSess_df.loc[appSess_df['TOTAL_SESSIONS_sum'] > 1]\n\n\n\ndf = pd.DataFrame(appSess_df2)\n\n# 2. Prepare data for Sankey diagram\n# Create a list of unique labels from 'Source_Category' and 'Target_Category'\nlabels = list(pd.concat([df['USER_NAME'], df['APP'],df['QUERY_TYPE']]).unique())\n\n\n# Map labels to indices, as Plotly's Sankey expects numerical indices for source and target\nlabel_to_index = {label: idx for idx, label in enumerate(labels)}\n\n# Create source, target, and value lists based on the DataFrame\nsources = df['USER_NAME'].map(label_to_index).tolist()\nsources = sources + df['APP'].map(label_to_index).tolist()\ntargets = df['APP'].map(label_to_index).tolist()\ntargets = targets + df['QUERY_TYPE'].map(label_to_index).tolist()\nvalues = df['TOTAL_SESSIONS_sum'].tolist()\nvalues = values + df['TOTAL_SESSIONS_sum'].tolist()\n#st.write(sources)\n#st.write(targets)\n#st.write(values)\n\n# 3. Create the Sankey diagram\nfig = go.Figure(data=[go.Sankey(     \n    node=dict(\n        pad=20,\n        thickness=100,\n        line=dict(color=\"black\", width=0.5),\n        label=labels,  # Use the unique labels for node labels\n        \n    ),\n    link=dict(\n        source=sources,\n        target=targets,\n        value=values,\n        label=labels,\n        \n    )\n)])\n\nfig.update_layout(title_text=\"User Sessions vs. App , vs. Query Type\",\n                  title_font_color=\"Black\",\n                  font=dict(size = 15, color = \"Black\",family=\"Arial\"),)\n                 #plot_bgcolor='black',\n                 #paper_bgcolor='black')\n# fig.update_layout(\n#     font=dict(\n#         family=\"Arial, sans-serif\",  # Specify a font family or preference list\n#         size=14,  # Adjust font size\n#         color=\"black\"  # Set font color\n#     )\n# )\nst.plotly_chart(fig)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b0af894b-949c-43f7-8732-bf2af0cdb6d1",
   "metadata": {
    "language": "sql",
    "name": "cache_query_history"
   },
   "outputs": [],
   "source": "select\n    -- query_history fields\n    qh.query_id, qh.query_type, left(qh.query_text,200) query_text, qh.query_parameterized_hash,\n    qh.warehouse_id, qh.warehouse_name, qh.warehouse_size, qh.warehouse_type, qh.cluster_number,\n    qh.query_load_percent, qh.query_acceleration_upper_limit_scale_factor,\n    qh.partitions_scanned, qh.query_acceleration_partitions_scanned, qh.bytes_scanned, qh.bytes_written,\n    qh.bytes_spilled_to_local_storage, qh.bytes_spilled_to_remote_storage,\n    qh.start_time, qh.end_time, qh.total_elapsed_time, qh.execution_time, qh.queued_overload_time,\n    qh.error_code, qh.user_name,\n    dateadd('millisecond',(\n        queued_provisioning_time + queued_repair_time + queued_overload_time +\n        transaction_blocked_time + compilation_time + list_external_files_time),\n        qh.start_time) as execution_start_time,\n    \n    -- credit attribution\n    qah.credits_attributed_compute, coalesce(qah.credits_used_query_acceleration,0) credits_used_query_acceleration,\n\n    -- query sizing fields\n    case when warehouse_type = 'STANDARD' THEN\n      decode(warehouse_size, 'X-Small', 1, 'Small', 2, 'Medium', 4, 'Large', 8, 'X-Large', 16, '2X-Large', 32, '3X-Large', 64, '4X-Large', 128, '5X-Large', 64, '6X-Large', 128, null)\n    else\n      decode(warehouse_size, 'Medium', 1, 'Large', 2, 'X-Large', 4, '2X-Large', 8, '3X-Large', 16, '4X-Large', 32, '5X-Large', 16, '6X-Large', 32, null) -- are these spo node counts right?\n    end as nodes_avail,\n    partitions_scanned / ((nullif(query_load_percent,0)/100) * nodes_avail) as files_per_node,\n    round((query_load_percent/100)*nodes_avail) as nodes_used,\n\n    -- sizing rules\n    -- case\n    --     when files_per_vcpu < 8 then -2\n    --     when files_per_vcpu < 64 then -1\n    --     when files_per_vcpu > 2000 then +2\n    --     when files_per_vcpu > 500 then +1\n    --     else 0\n    -- end as files_ind,\n    case\n        when files_per_node < 32 then -2\n        when files_per_node < 256 then -1\n        when files_per_node > 8000 then +2\n        when files_per_node > 2000 then +1\n        else 0\n    end as files_ind,\n    case\n        when bytes_spilled_to_remote_storage > 0 then 2\n        when bytes_spilled_to_local_storage > 0 then 1    -- for copy statements, bytes scanned might show up as spillage due to local caching\n        else 0\n    end as spill_ind,\n    case when query_load_percent < 100 then -1 else 0 end as qlp_ind,\n    -- case when query_acceleration_upper_limit_scale_factor > 0 then 1 else 0 end as qas_ind,\n    case when exists (\n        select query_id\n        from pst.svcs.query_acceleration_eligible\n        where query_id = qh.query_id\n          and start_time >= $start_time\n    ) then 1 else 0 end as qas_ind,\n    files_ind+spill_ind+qlp_ind+qas_ind as size_indicator,\n    case\n        when warehouse_size = 'X-Small' then iff(size_indicator > 0,'OVERSIZED','WELL-SIZED')\n        when nodes_used = 1 then 'SINGLE_NODE'\n        when size_indicator < 0 then 'UNDERSIZED'\n        when size_indicator = 0 then 'WELL-SIZED'\n        when size_indicator > 0 then 'OVERSIZED'\n    end as query_size,\n    decode(query_size,'SINGLE_NODE',1,'UNDERSIZED',2,'WELL-SIZED',3,'OVERSIZED',4) as query_size_sort\n    \nfrom pst.svcs.query_history qh\nleft join pst.svcs.query_attribution_history qah on qh.query_id = qah.query_id and qah.start_time >= $start_time\nwhere --qh.start_time >= $start_time\n  --and qh.start_time < current_date\n  qh.start_time > dateadd(day,-8,current_date)\n  and qh.cluster_number is not null\n  and qh.warehouse_size is not null\n  and qh.execution_time > 0;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "329d0183-4af1-40f3-918c-c91384f4642e",
   "metadata": {
    "language": "sql",
    "name": "wh_sizing_detail"
   },
   "outputs": [],
   "source": "select warehouse_size, query_type, query_size,\n    round(sum(credits_attributed_compute+coalesce(credits_used_query_acceleration,0))) as credits,\n    count(*) queries, count(distinct query_parameterized_hash) as query_hashes,\n    round(avg(execution_time)/1000,1) avg_exec_sec,\n    round(percentile_cont(.9) within group (order by execution_time)/1000,1) p90_exec_sec,\n    round(avg(total_elapsed_time-execution_time)/1000,1) avg_other_sec,\n    round(percentile_cont(.9) within group (order by total_elapsed_time-execution_time)/1000,1) p90_other_sec,\n    round(avg(query_load_percent)) as avg_qlp,\n    round(percentile_cont(.9) within group (order by query_load_percent)) p90_qlp,\n    round(avg(files_per_node),1) avg_files_per_node,\n    round(percentile_cont(.9) within group (order by files_per_node),1) p90_files_per_node,\n    round(avg(bytes_spilled_to_local_storage + bytes_spilled_to_remote_storage)/1024/1024/1024,1) avg_spill_gb,\n    round(percentile_cont(.9) within group (order by bytes_spilled_to_local_storage + bytes_spilled_to_remote_storage)/1024/1024/1024,1) p90_spill_gb,\n    sum(qas_ind) as qas_queries\nfrom {{cache_query_history}}\nwhere warehouse_name = $warehouse_name\ngroup by all\norder by credits desc;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "878f251d-ca45-4dd3-87c0-a15415b04389",
   "metadata": {
    "language": "sql",
    "name": "wh_sizing_workload"
   },
   "outputs": [],
   "source": "select query_parameterized_hash, any_value(query_text) as sample_query_text,\n    warehouse_size, query_type, query_size,\n    round(sum(credits_attributed_compute+coalesce(credits_used_query_acceleration,0))) as credits,\n    count(*) queries,\n    round(avg(execution_time)/1000,1) avg_exec_sec,\n    round(percentile_cont(.9) within group (order by execution_time)/1000,1) p90_exec_sec,\n    round(avg(total_elapsed_time-execution_time)/1000,1) avg_other_sec,\n    round(percentile_cont(.9) within group (order by total_elapsed_time-execution_time)/1000,1) p90_other_sec,\n    round(avg(query_load_percent)) as avg_qlp,\n    round(percentile_cont(.9) within group (order by query_load_percent)) p90_qlp,\n    round(avg(files_per_node),1) avg_files_per_node,\n    round(percentile_cont(.9) within group (order by files_per_node),1) p90_files_per_node,\n    round(avg(bytes_spilled_to_local_storage + bytes_spilled_to_remote_storage)/1024/1024/1024,1) avg_spill_gb,\n    round(percentile_cont(.9) within group (order by bytes_spilled_to_local_storage + bytes_spilled_to_remote_storage)/1024/1024/1024,1) p90_spill_gb,\n    sum(qas_ind) as qas_queries,\n    any_value(query_id) as sample_query_id\nfrom {{cache_query_history}}\nwhere warehouse_name = $warehouse_name\ngroup by all\norder by credits desc nulls last\nlimit 1000;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f5570912-27dd-40ff-8da0-987fd27843d6",
   "metadata": {
    "language": "sql",
    "name": "SessionsCounts"
   },
   "outputs": [],
   "source": "select USER_NAME, ROLE_NAME, COUNT(DISTINCT SESSION_ID) AS TOTAL_SESSIONS\nfrom\n    pst.svcs.QUERY_HISTORY\nwhere\n    start_time >= dateadd('day', -30, current_timestamp())\n    AND WAREHOUSE_NAME NOT ILIKE 'COMPUTE_%'\n    Group by ALL;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3721085b-75f9-46c0-a44d-ce592b727723",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "SessCount =  SessionsCounts.to_pandas()\nst.bar_chart(SessCount, x=\"ROLE_NAME\", y=\"TOTAL_SESSIONS\", color=\"USER_NAME\",horizontal = True)\n\n\n# Create an Altair bar chart\nchart = alt.Chart(SessCount).mark_bar(size=40).encode(\n    x='ROLE_NAME',\n    y='TOTAL_SESSIONS',\n    color = 'USER_NAME'\n)\n\n\n# Display the chart in Streamlit\nst.altair_chart(chart, use_container_width=True)",
   "execution_count": null
  }
 ]
}